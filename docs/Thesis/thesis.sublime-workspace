{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"aut",
				"autoref{label}	hyperref"
			],
			[
				"captio",
				"caption{title}	latex-document"
			],
			[
				"end",
				"end{minipage}	thesis.tex"
			],
			[
				"vd",
				"vdots	latex-document"
			],
			[
				"new",
				"newdata"
			]
		]
	},
	"buffers":
	[
		{
			"file": "chapters/02-literature-review.tex",
			"settings":
			{
				"buffer_size": 14493,
				"line_ending": "Unix"
			}
		},
		{
			"file": "chapters/05-evaluation.tex",
			"settings":
			{
				"buffer_size": 256,
				"line_ending": "Unix"
			}
		},
		{
			"file": "chapters/04-implementation.tex",
			"settings":
			{
				"buffer_size": 8588,
				"line_ending": "Unix"
			}
		},
		{
			"file": "chapters/06-conclusion.tex",
			"settings":
			{
				"buffer_size": 91,
				"line_ending": "Unix"
			}
		},
		{
			"file": "chapters/03-design.tex",
			"settings":
			{
				"buffer_size": 418,
				"line_ending": "Unix"
			}
		},
		{
			"file": "chapters/03-software-framework.tex",
			"settings":
			{
				"buffer_size": 38209,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "%!TEX root = thesis.tex\n\\chapter{Background} \\label{ch:background}\n\n% \\todo[inline]{Lots more citations, preferably state that it's someone else's work, cite and then write about it.}\n\nIn this chapter, laser scanning and the 3D-reconstruction pipeline are introduced. In section~\\ref{sec:heritage} the heritage preservation context for this work is laid out. In section~\\ref{sec:scanners}, 3d scanning is introduced and is followed by a description of the process by which raw scanner data transformed into a model in section~\\ref{sec:pipeline}. The chapter is concluded with a discussion of point cloud the cleaning process in section~\\ref{sec:cleaning}.\n\n\n\\section{Digital heritage preservation} \\label{sec:heritage}\n\nArchitectural heritage sites in many parts of the world are deteriorating or are under threat of destruction. The demolition of the Buddhas of Bamiyan by the Taliban in 2001 \\cite{Toubekis2009} is a recent example of such an event. Such threats highlight the need to digitally preserve heritage sites in order to keep a historical record after sites are gone or weathered. Preservation efforts have utilised many techniques to record buildings and ruins. Early efforts made use of tape measures and theodolites to produce simple ground plans. More recently, photogrammetry lets geomaticians produce 3D models \\cite{Heritage}. Laser range scanning is the latest technique that allows archivists to create extremely high resolution 3D models.\n\n % \\todo{Patrick: need more general background: projects? other todr? photo/text archiver? say why they are no longer used}\n\n% \\subsection{Laser scanners}\n\n% \\todo{expand, say more, justify focus on laser scanning}\n% \\todo[inline]{What is a range scanner, what is a point cloud, what is a range scan?}\n\n\\section{3D scanning} \\label{sec:scanners}\n\n3D scanners are optical devices that capture the shape, position and appearance of real world ofbjects. At the very least a 3D scanner will record coordinates of points on scanned surfaces. Gray scale or colour information may also be included. There are many types of 3D scanners on the market today. Each scanner has properties that make it more of less suitable for various scanning tasks. Generally, the size of the object and the level of detail one wishes to capture will determine ones choice of technology. Secondary factors such imaging speed, portability and cost of a scanner may also be important.\n\n3D scanning technologies can generally be classified into two categories, namely triangulation and time of flight scanners.\n\n\\subsection{Triangulation scanners}\n\nTriangulation scanners, as the name suggests, uses trigonometric triangulation \\cite{Frohlich2004} to locate surface points in a scene. Triangulation scanners are primarily available in two flavours, those that use laser light and those that use white light \\cite{Brown2012}. Laser triangulation scanners emit pulses that are reflected by an object and recorded by a sensor at a known position relative to the pulse origin. The sensor directly measures the angle of the returned beam that can then be used to compute the position of a point on an object surface. Light based scanners emit a series of linear patterns that similarly captured by a sensor at a known position. Unlike laser triangulation scanners, the sensor records perspective distortions in the reflected light patterns. These distortions are used to indirectly compute the angle of the returned light and determine the position of object surfaces.\n\n\\begin{figure}[H]\n	\\begin{subfigure}[b]{.33\\textwidth}\n	  \\centreing\n	  \\includegraphics[width=.9\\linewidth]{images/structured-light-scan}\n	  \\caption{}\n	\\end{subfigure}%\n	\\begin{subfigure}[b]{.33\\textwidth}\n	  \\centreing\n	  \\includegraphics[width=.9\\linewidth]{images/triangulate-laser-scan}\n	  \\caption{}\n	\\end{subfigure}\n	\\begin{subfigure}[b]{.33\\textwidth}\n	  \\centreing\n		\\begin{tikzpicture}[scale=2.5]\n\n		\\coordinate [label={[black]above:$Source$}] (A) at (0, 1);\n		\\coordinate [label={[black]above:$Sensor$}] (B) at (1, 1);\n		\\coordinate [label={[black]below:$Object$}] (C) at (0.5, 0);\n\n		\\draw[green] (A) -- node [above] {$c$} (B);\n		\\draw[black] (B) -- node [right] {$ $} (C);\n		\\draw[red] (C) -- node [left] {$b$} (A);\n\n		\\tkzMarkAngle[fill=white, size=0.25cm,%\n		opacity=1](C,A,B)\n		\\tkzLabelAngle[pos=0.15](C,A,B){$\\color{blue}A$}\n\n		\\tkzMarkAngle[fill=white,size=0.25cm,%\n		opacity=1](A,B,C)\n		\\tkzLabelAngle[pos=-0.15](C,B,A){$\\color{orange}B$}\n\n		% \\tkzMarkAngle[fill=green,size=0.25cm,%\n		% opacity=.4](B,C,A)\n		% \\tkzLabelAngle[pos= 0.15](B,C,A){$ $}\n\n		\\end{tikzpicture}\n		\\caption{}\n	\\end{subfigure}\n	\\caption{Triangulation 3D scanners \\protect\\footnotemark}\n	\\label{fig:triangulation-scanners}\n\\end{figure}\n\nGiven the precise distance between the light source and sensor ($\\begingroup\\color{green}c\\endgroup$) and as well as the outgoing angle of emitted light ($\\begingroup\\color{blue}A\\endgroup$) and incoming angle of reflected light ($\\begingroup\\color{orange}B\\endgroup$), the distance to the object is given by $\\begingroup\\color{red}b\\endgroup = \\begingroup\\color{green}c\\endgroup\\frac{ sin(\\begingroup\\color{orange}B\\endgroup)}{sin(\\pi - \\begingroup\\color{blue}A\\endgroup-\\begingroup\\color{orange}B\\endgroup)}$.\n\n\\subsection{Time of flight scanners (TOF)} \\label{subsec:tof}\n\n\\begin{figure}[ht]\n  \\centreing\n  \\includegraphics[width=.5\\linewidth]{images/pulse-tof}\n  \\caption[Time of flight scanner]{Time of flight scanner \\protect\\footnotemark[\\value{footnote}]}\n  \\label{fig:tof}\n\\end{figure}\n\\footnotetext{Source: \\url{http://www.rapidform.com/3d-scanners/}}\n\nTime of flight (TOF) scanners emit laser pulses just like triangulation laser scanners. Unlike triangulation scanners, time of light scanners measure the time taken by a pulse to reflect to be detected by the scanner's sensor (see \\autoref{fig:tof}). The distance travelled by the pulse can be computed by considering the speed of light. Given the time it takes for a pulse to travel to a surface and back to the scanner ($t$), the distance ($d$) is given by $d = ct/2$ where $c$ is the speed of light. The accuracy of the distance calculation depends on how precisely time can be measured \\cite{Form2014}.\n\n\\begin{figure}[ht]\n	\\begin{tikzpicture}[scale=1]\n	\\begin{axis}[\n		width=1\\textwidth,\n		height=150,\n	    axis lines = left,\n	    yticklabels={,,},\n	    xticklabels={,,},\n	    axis x line=centre,\n	    xlabel = $Time$,\n	    ylabel = {Amplitude},\n	    legend style={\n	    at={(1,0)},\n	    anchor=south east}]\n	]\n	\\addplot[samples=500,domain=0:2*pi, color=red]{sin(deg(x))};\n	\\addlegendentry{Transmitted signal}\n	\\addplot[samples=500,domain=0:2*pi, color=blue]{sin(deg(x-pi/2))};\n	\\addlegendentry{Received signal}\n	\\draw[<->] (axis cs:2.61799,0.5) -- node[above]{Phase shift $t$} (axis cs:4.18879,0.5);\n	\\end{axis}\n	\\end{tikzpicture}\n	\\caption{Phase shift in returned signal}\n	\\label{fig:phase-shift}\n\\end{figure}\n\nLaser phase-shift scanners \\cite{Frohlich2004} are a variation on the standard TOF scanner. These scanners modulate the power of the laser pulse in a sinusoidal wave during a scan. The phase shift in the returning pulse is then used to compute the round trip time (see \\autoref{fig:phase-shift}). Due to the cyclical nature of the signal, phase shift is ambiguous. This ambiguity is resolved by using measurements at multiple frequencies \\cite{Bhurtha}.\n\n\n\\subsection{Comparison}\n\nLaser and structured light triangulation scanners can achieve 10 micrometer accuracy over distances less than one meter. Over longer distances, however, the triangle in \\autoref{fig:triangulation-scanners} becomes elongated which results in more variance in the distance calculation \\cite{Mackinnon2006} when compared to TOF scanners. Structured light scanners specifically are less prone to motion distortions when compared to laser scanners and tend to also be faster \\cite{Brown2012}. Hand-held scanners triangulation are easy to set-up and use compared to other scanners. Accuracy, speed over short distances in combined with their low cost and portability make these scanners preferable for objects that can be scanned at short range.\n\nThe accuracy of TOF scanners is a function of how accurately a laser pulse round trip time can be measured. TOF scanners tend to be less accurate over short distances (typically less than 2 meters) as there is less distance over which to amortize measurement error. Larger distances up to 1 km can however be measured with high accuracy \\cite{Form2014}. The speed of TOF scanners depends on the resolution required. Tens of thousands of points may take seconds while resolutions with millions of points may take minutes.\n\nPhase-based scanners have a more limited range due to the cyclic nature of the sinusoidal pulse \\cite{Bhurtha}. Objects from beyond the scanner's designed range can sometimes be erroneously interpreted as being far closer when the scanner fails to disambiguate pulses. Phase based scanners compensate for this disadvantage by being much faster and more accurate than traditional TOF scanners within a shorter range (2-100m) \\cite{Form2014}.\n\n\n\\begin{table}\n\\begin{tabular}{ |l|l|l|l|l|l| }\n  \\hline\n  Type &              Range &        Precision       & Speed & Portability \\\\\n  \\hline\n  Structured light &    <1m     & 10 micrometer  & Seconds & High \\\\\n  Laser triangulation & <1m     & 10 micrometer  & Seconds  & Medium \\\\     \n  TOF &                 2-1000m & Medium      & Minutes & Low \\\\\n  Phase &               2-100m & Low         & Seconds to Minutes & Low \\\\\n  \\hline  \n\\end{tabular}\n\\caption{Comparison of scanning technology}\n\\end{table}\n\n\n\\subsection{Range images} \\label{sec:data}\n\n\\footnotetext{Source: \\url{http://www.rapidform.com/3d-scanners/}}\n\n\\begin{figure}[ht]\n  \\centreing\n  \\includegraphics[width=1\\linewidth]{images/grid}\n  \\caption{2D scan grid of intensity values \\protect\\footnotemark[\\value{footnote}]}\n  \\label{fig:grid}\n\\end{figure}\n\n\\footnotetext{Rendering based on data provided by the \\href{http://www.zamaniproject.org/}{Zamani Project}}\n\nThe terrestrial laser scanner technology described above produces angle and range measurements that are converted into 3D coordinates relative to the scanner and returned as a range image. A range image is a two dimensional array of samples, that in addition to 3D coordinates, may also include the intensity of the returned laser pulse, as well as colour measurements, and other information specific to a scanner or manufacturer \\cite{Frohlich2004}. We limit our investigation to range images produced by TOF and phase scanners that return an intensity value in addition to coordinates with every point. We exclude colour information as such scans were not readily available to us at the time of writing.\n\nA range image is considered to be 2.5D rather than full 3D as it is limited to representing objects captured from a single perspective. When viewed in 3D, a range scan will contain many occluded objects. One will also notice the sampled point density reduces as one moves away from the scan origin. This can make it hard to identify objects in far away low density regions of a range image. Rendering a range scan as a grey scale 2D panoramic image (see \\autoref{fig:grid}) may make objects more recognizable. The grid structure of range scans can also facilitate cheap neighbour searches (discussed later).\n\n\\section{3D reconstruction pipeline} \\label{sec:pipeline}\n\n% \\todo{Used many of the same refs, find some others}\n\n\\begin{figure}\n\n	% Define block styles\n	\\tikzstyle{decision} = [diamond, draw, fill=blue!20, \n	    text width=4.5em, text badly centreed, node distance=3cm, inner sep=0pt]\n	\\tikzstyle{block} = [rectangle, draw, fill=gray!20, \n	    text width=5em, text centreed, rounded corners, minimum height=4em]\n	\\tikzstyle{optionalblock} = [rectangle, draw, dashed,\n	    text width=5em, text centreed, rounded corners, minimum height=4em]\n	\\tikzstyle{line} = [draw, -latex']\n	\\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,\n	    minimum height=2em]\n	\n	\\centreing\n\n	\\begin{tikzpicture}[node distance = 2cm, auto]\n	    % Place nodes\n	    \\node [block] (acquire) {Scan acquisition};\n\n	    \\node [optionalblock, below right of=acquire, node distance=3cm] (clean) {\\bf{Clean}};\n	    \\node [block, below left of=acquire, node distance=3cm] (register) {Register};\n	    \\node [block, below of=acquire, node distance=4cm] (reconstruct) {Reconstruct surface};\n	    \\node [optionalblock, right of=reconstruct, node distance=3cm] (autofill) {Automatic hole filling};\n	    \\node [optionalblock, left of=reconstruct, node distance=3cm] (filling) {Manual hole filling};\n	    \\node [block, below of=reconstruct, node distance=2cm] (texture) {Texture};\n	    % Draw edges\n	    \\path [line] (acquire) -- (clean);\n	    \\path [line] (acquire) -- (register);\n	    \\path [line][<->] (clean) -- (register);\n	    \\path [line] (register) -- (reconstruct);\n	    \\path [line][-] (reconstruct) -- (autofill);\n	    \\path [line] (reconstruct) -- (filling);\n	    \\path [line] (filling) -- (texture);\n	    \\path [line] (reconstruct) -- (texture);\n\n	    \\draw[densely dotted] (1.7,-5.3) rectangle (4.7,-6.7);\n\n	    % legend\n	    \\node [block, right of=texture, node distance=2.5cm, scale=0.7,\n	    	text width=3.8em, minimum height=1em](required) {Required step};\n	    \\node [optionalblock, right of=required, node distance=1.4cm, scale=0.7,\n	    	text width=3.8em, minimum height=1em] (notrequired) {Optional step};\n\n	\\end{tikzpicture}\n\n	\\caption{Reconstruction pipeline \\protect\\footnotemark[\\value{footnote}]}\n	\\label{fig:pipeline}\n\n\\end{figure}\n\\footnotetext{Adapted from \\citet{Ruther2011}}\n\n3D reconstruction of a site starts with the acquisition of range scans in the field. The collection of raw scans are then subjected to a series of processing steps. Unwanted objects and noise are typically removed and missing data can be synthesized. The scans are then transformed into a common coordinate frame in a process called registration. A surface model can then be constructed from the registered point sets. The final model is complete when the reconstructed mesh is textured (see \\autoref{fig:pipeline}) \\cite{Ruther2011}.\n\n\\subsection{Data acquisition}\nSome degree of planning is required to ensure the successful 3D acquisition of a heritage site. One needs to decide on an appropriate level of scan detail, as the set resolution of a scanner determines the data acquisition rate. Scanner positions that result in flat angles of incidence with surfaces should be avoided to maximize sample density and surface detail. Good scanning positions will ensure that maximum site coverage is achieved in the most economical way.\n\nAdditional considerations can speed up later steps in the pipeline. Automated registration tools require that that overlap between scans is achieved. Ensuring that the scanner is consistently at the same level and orientation can simplify registration \\cite{Ruther2011}.\n\n\\subsection{Registration}  \\label{sec:registration}\n\n\\begin{figure}[ht]\n  \\centreing\n  \\includegraphics[width=0.6\\linewidth]{images/registration}\n  \\caption{ICP correspondences \\protect\\footnotemark.}\n  \\label{fig:registration}\n\\end{figure}\n\\footnotetext{Source: \\url{http://pointclouds.org/documentation/}}\n\nTwo approaches are commonly used to transform all scans into a common coordinate system. The first approach requires placing reference objects, called targets, around a site during data acquisition. When the same object is found in two scans, the object's target can be used to align the scans \\cite{Besl1992}. The second approach, Iterated Closest Point (ICP) aligns scans by using surface features \\cite{Bernardini2002}.\n\nAutomated systems easily identify targets and provide highly accurate registration. Targets do, however, have to be captured in high resolution in order to be useful. This may require one to capture more scans than required for simple documentation purposes. In the heritage domain, targets have been found to be impractical as they often need to be placed in hard to reach places and intricate indoor environments require too many targets \\cite{Ruther2011}.\n\nICP requires less effort during data acquisition. However the registration procedure requires that two surfaces are roughly aligned as an initial condition, usually by hand. Correspondences are then computed between the surfaces and an incremental transformation is then computed to minimize the distance between all the correspondences. After the transformation is applied, the procedure is repeated until convergence is reached \\cite{Rusinkiewicz}. There are many versions to the ICP algorithm that vary mainly in the type of correspondent matching and optimization procedure used.\n\nA user needs to ensure a good initial alignment or convergence may not be achieved. Featureless surfaces are also problematic as the optimization problem will have multiple solutions which may lead to misalignment.\n\n\\subsection{Cleaning}\\label{sec:cleaning}\n\n\\begin{figure}[ht]\n	\\begin{subfigure}[b]{.5\\textwidth}\n	  \\centreing\n	  \\includegraphics[width=.9\\linewidth]{images/dirty}\n	  % \\caption{Structured light 3D scanning \\cite{Form2014}}\n	  % \\label{fig:structured-light-scan}\n	\\end{subfigure}%\n	\\begin{subfigure}[b]{.5\\textwidth}\n	  \\centreing\n	  \\includegraphics[width=.9\\linewidth]{images/clean}\n	  % \\caption{Laser 3D scanning}\n	  % \\label{fig:images/triangulate-laser-scan}\n	\\end{subfigure}\n  \\caption{Trees cleaned from a scan}\n  \\label{fig:cleaning}\n\\end{figure}\n\nCleaning is considered an optional step for documentation purposes. However, skipping it in the modelling pipeline can lead to poor results during surface reconstruction (see Section \\ref{sec:reconstruction}). The aim of cleaning is to remove trees, people, cables, cars, doors, animals and other unwanted objects, as well as scanner artefacts.\n\nAn object is usually removed either because it is not part of the subject matter or it won't mesh well later in the pipeline. Cars parked near monuments or power lines are frequently removed. Trees or other vegetation may be considered as part of the subject matter but are often removed because meshing algorithms do not produce good results when they are present.\n\nArtefacts are unwanted points that result from of imperfect equipment rather than physical surfaces. Distant samples that are interpreted as being close by in phase-based TOF scanners are one such instance (see Section \\ref{subsec:tof}). Incorrectly binned points will appear normal in a 2d rendering but will look like noise in 3D. The sun or other light sources can also result in artefacts. Noisy artefacts can also be the result of fog or smoke in an environment \\cite{Ruther2011}.\n\n% \\todo[inline]{More detail on artifacts}\n\n\\begin{figure}[ht]\n  \\centreing\n  \\includegraphics[width=1\\linewidth]{images/mixed-pixel}\n  \\caption{Mixed pixels. Green are valid points and red are not. \\cite{Tuley2005}}\n  \\label{fig:mixed-pixel}\n\\end{figure}\n\nMixed pixels is phenomenon that can occur when a laser pulse partially strikes a nearby and distant object. The result is an interpolated data point between the near and far object \\cite{Tuley2005} (see \\autoref{fig:mixed-pixel}). This typically manifests as column of points behind doorways or close to other edges. \\\\\n\nCleaning scan data is typically a manual task requiring expert judgement. It can be performed on individual scans before registration or after registration \\cite{Ruther2011}.\n\nCleaning single scans before registration typically requires less system resources as less data needs to be loaded. Loading multiple large scans can be problematic on less capable hardware. One may, however, choose not to remove all unwanted objects as some objects may necessary for registration. A tree on a flat surface may be necessary for correspondence matching in ICP. Without these objects automated registration may not converge on feature-sparse surfaces. Registration and cleaning may thus, in practice, be an iterative process. Removing objects from the registered set of range images may be easier due to the following. An increased number of samples from multiple perspectives can make objects easier to recognize. It may also be possible to remove an unwanted set of points from multiple scans in one action. System resource constraints can be managed by loading one or more scans at a time.\n\nAfter registration, all range images are combined into a single large point cloud. The combined dataset is typically too large to fit into memory and few software packages can support data sets of this size. It is possible to clean the combined point cloud by loading slices. When doing so one does not have the benefit of referencing the 2D range image when cleaning the combined data set. One would also have trouble mapping the result back to the original scans for archival purposes. The 2D array structure of range images also supports performing constant neighbour lookups. If this is not done $O(log(n))$ octrees or kd-trees will be necessary which can slow down feature computations.\n\nSome large heritage scanning projects have reported typical scans taking from 30 minutes up to 120 minutes to clean by an experienced person \\cite{Ruther2011}. High resolution scans can take up to a full day to clean. Existing tools leave much to be desired in terms of cleaning efficiency. More detail is presented in section \\ref{sec:interactive-tools}.\n\n\\subsection{Surface reconstruction}  \\label{sec:reconstruction}\n\n% \\begin{figure}[ht]\n%   \\centreing\n%   \\includegraphics[width=0.6\\linewidth]{images/mls}\n%   \\caption{Surface fitting \\protect\\footnotemark.}\n%   \\label{fig:registration}\n% \\end{figure}\n% \\footnotetext{Source: \\url{http://en.wikipedia.org/wiki/File:Linear_regression.svg}}\n\nSurface reconstruction converts the discrete point model into a triangulated surface model. This can be achieved either via interpolation methods, that aim to connect neighbouring points by computing a triangulation, or by approximation methods that aim to approximate a surface that fits the samples \\cite{Schall2005}.\n\nInterpolation methods are very sensitive to noise. Poor triangulations are achieved when scans are not properly registered or the sampling exhibits high variance. A preprocessing step can mitigate these problems but tends to be time consuming and often leads to a loss of detail \\cite{Ruther2011}.\n\nSurface approximation algorithms are less susceptible to noise. Poisson surface reconstruction \\cite{Kazhdan2006} is a popular method that is noise resistant and achieves great detail. It uses samples with normals to interpolate a normal field and then solves a Poisson equation to compute an indicator function. Results are dependent on good normal estimations. It is thus preferable to compute normals prior to registration when using this approach. Poisson surface reconstruction produces ``water-tight'' surfaces and automatically fills small holes. While this is useful for producing a visually pleasing 3D model, it does not result in a historically accurate model because new surface data may be synthesized in the process.\n\nMoving Least Squares (MLS) is another popular surface approximation method that does not over smooth and interpolate missing data \\cite{Alexa2003}. Like Poisson, it also requires normal estimates. MLS computes a local surface approximation at a sample point by considering its neighbourhood. Every point in a neighbourhood is weighed according to its importance. A surface is then computed by minimizing the weighed distance to the surface for each point in the neighbourhood. Poisson and MLS have both been adapted for out-of-core execution and GPU acceleration \\cite{Merry2014}. \\todo{Discuss issues else it may look perfect}\\\\\n\n\n\\subsection{Hole filling} \\label{sec:filling}\n\nIt is unlikely that a scanning expedition will achieve complete coverage of a site. Furthermore, points are removed during cleaning and reconstruction of the surface. Hole filling is an optional step in the reconstruction pipeline that seeks to synthesize missing data \\cite{Sharf2004} \\todo{Expand}. For historical accuracy, hole filling is not desirable as data is synthesized. However, models that have been filled are more aesthetically pleasing. If holes are filled it is thus important to know what data has been synthesized.\n\nSmall holes can be filled automatically by surface reconstruction algorithms (in discussed in \\ref{sec:reconstruction}). Larger patches are harder to fill convincingly with automated methods and may require manual effort to produce good results \\cite{Ruther2011}.\n\n\n\\subsection{Texturing} \\label{sec:texturing}\n\nThe final, but optional, step of the reconstruction pipeline is texturing. Colour in a model is not only visually pleasing but also form part of the historical record. Some laser scanners are equipped with cameras to capture colour information for each sample. Although this is very convenient, some in the heritage community have reported that poor texturing is achieved when compared to external cameras \\cite{Ruther2011}. The biggest problem when texturing a model is dealing with different light conditions during the day. Capturing photos around the same time of day across multiple days is one way to mitigate the problem.\n\nA model is textured from a series of photos by projecting the images onto the model. In order to achieve this, internal and external camera parameters needs to be known from time the picture is taken. This includes the position and orientation of the camera as well as the focal length of the lens. If this is not known it can be estimated via software by selecting or computing correspondences between pictures \\cite{Ruther2011}.\n\n% \\todo{more citations on texturing, ask rudy}\n\n\n\\section{The cleaning problem}\n\nIn the previous section the point cloud cleaning task was framed in the context of the 3D reconstruction pipeline. In order to set clear goals it is important understand what point cloud cleaning entails.\n\n% In this section we will look at the current state of point cloud segmentation tools and frame the point cloud segmentation problem.\n\n% In the previous section 3D reconstruction from laser scans in a heritage preservation context was discussed. Environmental and instrument noise in laser scans are hard to remove and can be taxing in terms of human resources. The goal of this research is to speed up the cleaning process through tools and semi automated methods. Given the noise characteristics and properties of the point cloud data sets made available the problem is outlined below.\n\n\n\\subsection{Tools}\n\n\\begin{table*}[htb]\n\\centreing\n  \\begin{tabular}{| l | x{3cm} | x{3cm} | c |}\n	\\hline & Interactive tools & Filters & Open source \\\\    \n    \\hline\n	\n		Terrascan \\cite{Terrasolid2012} &\n		&\n		ground points, vegetation, buildings &\n		\\tickNo \\\\\n\n	\\hline\n		Pointools Edit \\cite{Pointools2012} &\n		lasso select, rectangle select, ball and cube brush select, floodfill with distance threshold, floodfill based on colour or intensity similarity, plane select &\n		&\n		\\tickNo \\\\\n	\n	\\hline\n		VR Mesh Studio \\cite{VirtualGrid2012} &\n		power lines &\n		ground, vegetation, roofs, planes &\n		\\tickNo \\\\\n	\n	\\hline\n		Carlson PointCloud \\cite{Carlson2012} &\n		&\n		clear isolated and duplicate points, extract bare earth &\n		\\tickNo \\\\\n	\n	\\hline\n		3D Reshaper \\cite{Technodigit2012} &\n		lasso select &\n		clustering with distance metric, clustering colors, remove isolated points &\n		\\tickNo \\\\\n	\n	\\hline\n		Cyclone \\cite{Leica2012} &\n		lasso select, floodfill based on smoothness similarity &\n		&\n		\\tickNo \\\\\n	\\hline	\n		Meshlab \\cite{VisualComputingLaboratory2012} &\n		point picking, plane select &\n		isolated point removal &\n		\\tickYes \\\\\n	\\hline	\n		Cloud Compare &\n		point picking, polygon select, rectangle select &\n		&\n		\\tickYes \\\\\n	\\hline\n  \\end{tabular}\n  \\caption{Existing systems}\n  \\label{table:software}\n\\end{table*}\n\n\n% As efficiency of a tool increases the accuracy  \n\n\n% Object recognition is a difficult computer vision problem that humans can often solve instantly. State of the art vision algorithms can be computationally expensive and need to be trained to perform recognition tasks. \n\n% The problem can be approached by a human or machine.\n\n% The distinction between wanted and unwanted points is somewhat subjective and dependent on one's interpretation of the data. Object recognition is an essential part \n\n% is to group points according to their properties. At a very high \n\n% An ideal solution would be a fully automated and require no human interaction. It is however sometimes difficult, even for humans, to unambiguously determine whether a point is part of an important structure or simply noise. Different people may segment the same scene differently. It is unlikely that an automated solution would be satisfactory in all cases due a large degree of classification subjectivity between people. \\todo{Cite a hard problem in vision to justify this?}\\\\\n\n% We will not attempt a fully automated solution to the problem but rather frame it as an interactive segmentation problem. The user is thus at the centre of the cleaning process. The goal is to provide a set of tools that will speed up or automate parts of the task while maintaining accuracy.\n\n\nPoint cloud cleaning is in essence a segmentation problem. The high level goal is to divide a point set into wanted and unwanted points. In order to make this distinction it is important to recognize what points in a scene represent. Humans are very good at object recognition which is part of the reason why point cloud cleaning has remained a very manual task. Despite this general proficiency, software interfaces often make it hard to group points according to their perceived class.\n\nIn order to facilitate efficient point cloud cleaning, or general point cloud editing, software packages provide a range of tools. Tools that do more tend to perform tasks less accurately. In order to be effective, a system should provide tools with both large and small areas of influence so that crude segmentations may be refined and the desired accuracy be obtained with minimal effort. In lieu of a fully automated solution it is also important that tools be computationally efficient so that interactivity is maintained.\n\n\n% The most accurate tool would let a user label a single point at a time. At this level of accuracy it might take a long time to complete a segmentation task. On the opposite side of the spectrum, a tool could automatically label all points. This could maximize efficiency but would likely not be very accurate as object recognition is a difficult computer vision problem.  State of the art vision algorithms can be computationally expensive and need to be trained to perform recognition tasks.\n\n\n\n\n% When offloading part of the segmentation task to semi-automated tools, it is important to allow the user refine the result. It is thus necessary to provide more accurate manual tools to correct possible mistakes. Because the user is expected to refine semi automated segmentations, it is important for semi automated tools to be fast.\n\nA number of open source and commercial software point cloud packages are available (see \\ref{table:software}). As the source code and technical descriptions of proprietary packages are not available, they will be treated as black boxes. The software packages surveyed contain two types of tools which we shall refer to as interactive tools and filter tools. Interactive tools provide immediate feedback and are driven primarily via mouse and keyboard input. Filter tools on the other hand are controlled via parameters and operate a set of points that are specified prior to use.\n\n\\subsubsection{Interactive tools}\\label{sec:interactive-tools}\n\nThe most basic interactive tool is \\emph{point picking} which allows one to select a single point. It is not very effective for cleaning purposes. \\emph{Meshlab} \\cite{Cignoni2008}, an open source mesh manipulation program, includes this tool. It is more likely to be useful when editing meshes.\n\nThe \\emph{polygon select} tool is one of the more useful interactive tools. The tool lets the user select points by constructing a 2D polygon around a set of points on the view port. It is useful when selecting points that are isolated in space. Polygon selection becomes problematic when one is unable find a perspective that separates the target points from its neighbours. \\emph{Cyclone} is a proprietary package \\cite{Leica2012} that solves this problem by restricting the tools area of influence with a limit box. Cyclone also allows a user TO pan the view port while constructing a polygon. Meshlab has a similar tool that is limited to rectangular areas. \\emph{Cloud Compare} \\cite{CloudCompare} is another open source package that provides both a polygon and rectangular select version of the tool.\n\n\\emph{Brush tools} are another class of interactive tools that lets one select points by painting on the view port. The `brush' shape is typically a sphere but packages like \\emph{Pointools Edit} \\cite{Pointools2012} also provide additional shapes such as a cube. Brush tools has the same disadvantage as the polygon select tool when it comes to selecting points that cannot be isolated via perspective.\n\n\\todo{State that we overcome this by only operating on layers}\n\n\\begin{figure}[ht]\n  \\centreing\n  \\includegraphics[width=1\\linewidth]{images/plane}\n  \\caption{Plane selection tool in Pointools \\cite{Pointools2012}}\n  \\label{fig:plane-select}\n\\end{figure}\n\nInteractive \\emph{flood fill} tools not unlike those found in raster image editors are also common in point cloud packages. The tool labels points by growing a region from a user selected seed point. Various membership criteria can be specified in order to limit the selection. Pointools \\cite{Pointools2012} allows a user to restrict the selection by specifying a threshold in terms of colour or intensity value. Pointools as well as Meshlab includes a variation that selects planes (see \\autoref{fig:plane-select}). Cyclone \\cite{Leica2012} provides a smoothness constraint that lets one select regular surfaces. More specialized flood fill tools can perform power line removal, such as the tool found in \\emph{VR Mesh Studio} \\cite{VirtualGrid2012}. The success of a flood fill tool depends on good threshold being set. If the user is responsible for specifying these values it may be hard to use for less sophisticated users. Meshlab uses a novel technique that allows a user to limit the extent of the flood fill by dynamically controlling a threshold via the mouse wheel. This is used in both the Meshlab plane selection tool and the vertex clustering tool (which uses the distance from the original selection as a threshold).\n\n\\subsubsection{Filters} \\label{section:filters}\n\nFilters can range from simple denoising functions to complex machine learning algorithms. Software packages generally allows one to restrict a filter's area of influence.\n\nMeshlab provides a number of denoising filters. \\emph{Outlier removal} allows a user to specify the number of neighbours a point is required to have in order to not be removed. This tool is problematic when used with range images due to the decreasing point density away from the scan origin. Points close to the origin are less likely to be classified as outliers due to the higher point density while false positives are more common further away where there are fewer samples.\n\n\\begin{figure}[ht]\n  \\centreing\n  \\includegraphics[width=1\\linewidth]{images/clustering}\n  \\caption{Clustering issue in 3D Reshaper \\cite{Technodigit2012}}\n  \\label{fig:clustering-issue}\n\\end{figure}\n\n\\emph{Clustering} tools are also affected by non uniform density. \\emph{3D Reshaper} \\cite{Technodigit2012} provides a filter that groups spatially related points. This approach work fine on uniform point clouds. However, range scans poses a problem as more clusters are detected further away from the origin (see \\autoref{fig:clustering-issue}).\n\n\\begin{figure}[ht]\n  \\centreing\n  \\includegraphics[width=1\\linewidth]{images/vrmesh-veg}\n  \\caption{Vegetation filter in VR Mesh Studio \\cite{VirtualGrid2012}}\n  \\label{fig:vegetation-issue}\n\\end{figure}\n\n\nMore specialized filtering tools exist, including ones aimed at extracting the ground plane \\cite{Terrasolid2012, VirtualGrid2012}, finding rooftops \\cite{VirtualGrid2012}, walls \\cite{VirtualGrid2012}, buildings \\cite{Terrasolid2012} and vegetation \\cite{Terrasolid2012}. Most specialized filters are aimed at the surveying community who have different needs to those working in heritage domain. Eroded old structures are not easy to segment using tools designed to segment modern buildings that are more regular in shape. The heritage preservation community also has a strong emphasis on accuracy. Segmentations that may be satisfactory in other domains, such as surveying, may be far to crude for people  the heritage domain. Vegetation removal is one such instance. VR Mesh Studio's \\cite{VirtualGrid2012} filter for segmenting vegetation detects plant growth on eroded ruin walls (see \\autoref{fig:vegetation-issue}). False positives can be tolerated when searching for vegetation in a scene. It is is more problematic when the goal is to remove the identified points.\n\n\\subsection{Interface}\nA user ultimately needs to judge whether a point is wanted or not. It is thus important for the system to present the data in a way that is easy to interpret and easy to interact with. Failing to do so will make cleaning more difficult.\n\nMeshlab and Cloud Compare violate the important usability heuristic by not including an undo feature \\cite{Nielsen2005}. Cleaning is a time consuming task and making a mistake can derail hours of work. The lack of an undo function can also slow a user down by requiring greater caution \\cite{Miller1977}. Propriety packages tend to be more usable in this regard.\n\n\n% Familiarity helps new users quickly get started. It is thus worth while looking at the interfaces of existing systems.\nThe tools surveyed all presented the user with an interactive 3D rendering of a point cloud. In addition to this Z+F \\cite{Z+F2014} provides users with a 2D range image view. Humans are more adept at interpreting 2D data despite our environment being 3D \\cite{Livingstone1987}. In order to perceive depth on a screen, an object has to move \\todo{Cite, move or stereo vision, Patrick: depth cues}. Given that a range scan does not reveal more information as one's 3D perspective shifts, it is questionable whether a 3D view is best suited for range scan segmentation. \\note{Depth aware fill fixes this for 2D}\\\\\n\nInterfaces typically provide two ways to navigate a 3D scene. One can either transform the point cloud (object centric navigation), or manipulate the camera (camera centric navigation). All systems surveyed implemented object centric navigation. Cloud Compare and Meshlab amongst others render a track ball around the centre of rotation. This makes it easier to reason about the interaction, especially when the centre of mass is not at the scan origin. Cloud Compare and other systems lets a user set the centre of rotation. In Meshlab, the centre of rotation is fixed at the coordinate system origin. Object centric navigation is more intuitive when viewing movable objects. Environments however, are typically viewed from an upright perspective. Moving in and around a 3D environment from a 1st person perspective is more natural than moving the environment around. A rotated heritage site is not a familiar perspective and is likely to disorient a user.\n\nSome systems use lighting when rendering the the point cloud. Meshlab lets one toggle lighting on and off. With lighting switched on the interface becomes less responsive when viewing large data sets. Lighting may help interpret a scene when viewing point clouds with no intensity or color information. However, when this information is available, such as with our data sets, it is questionable whether any value is added as artificial lighting may conflict with shadows captured at the time of the scan.\n\n\n\\section{Summary}\nIn this chapter a high level overview of digital the heritage preservation process was presented. Removing instrument noise and unwanted objects from range scans is a difficult and time consuming part thereof. Existing software packages provides a range of tools that aim to make point cloud manipulation easier. The tools that automate the segmentation of large regions, however, have issues dealing non uniform density range images and eroded heritage structures. Open source point cloud editing tools are few and are less capable than their propriety counterparts. Introducing equally or more advanced tools into open source frameworks may improve user productivity. However, systemic usability problems, such as the lack of undo, will hold users back. To address these issues, we introduce a newly created open source range image cleaning framework in the next chapter. This framework takes into account usability concerns and will provides a solid baseline on which to build the more effective cleaning we develop tools in later chapters.\n\n\n% Terrestrial laser scanning is the primary tool used in digital heritage preservation for the documentation and 3d reconstruction. Triangulation scanners are more suited for scanning over short distances while TOF scanners work best for longer ranges. Traditional TOF scanners are slow but the phase based variation improves in speed while sacrificing range and potentially introducing noise.\n\n% Only TOF scanners are considered in this research. The data set returned from a TOF scanner varies between manufacturers. Datasets used here are in a grid structure and contain X, Y, Z coordinates as well the intensity of the reflected laser pulse. Manufacturers may also associate color with each point as well as raw scan data.\n\n% Laser scanner data sets need to be processed to create 3D models. This process includes cleaning, registration, meshing, hole filling and texturing.\n\n% During cleaning there are two types of noise that needs to be removed: instrumentation noise created by the scanner and environmental noise that is created by physical objects in the scene. Scanner manufacturers typically provide software that semi automatically remove instrumentation noise with varying levels of success. Removing environmental noise however is typically a more manual endeavor. The cleaning task is a very time consuming part of the reconstruction pipeline.\n\n% In this work we attempt to find ways to speed up the cleaning process through tooling and semi automated methods. We focus this on methods applicable to TOF scans in a grid form with X, Y, Z and intensity values.\n\n\n% \\subsubsection{Segmentation}\n\n% In the sections above we presented a high level overview of the point cloud cleaning task and the tools available to complete it. We now approach segmentation from the bottom up and introduce the different types techniques we can apply to tackle separate wanted from unwanted points.\n\n% The goal of segmentation is to partition a scene into meaningful subsets. There are many possible partitions for any range scan. To produce meaningful segments we need a model of what needs to be segmented based on prior world knowledge. \n\n% In some instances it may be simple segment to a scene by simply using raw data. If we made the observation that tree bark absorbs laser light more than other objects in a scene, we could use the intensity of the laser light captured by the scanner to segment local neighborhoods of tree bark. If we were trying to segment an entire tree instead of just the bark, it may be hard to express a model in terms of raw range scan data. Abstracting the data may make it easier to express a model. Feature extraction, clustering and classification are ways that can help us simplify the segmentation processes by creating higher level abstractions to work with.\n\n\n% \\subsubsection{Features}\n\n% A feature is a loosely defined term used in many disciplines including statistics, machine learning and computer vision \\cite{Flach2012}.\n% Features are generally floating point or integer values but can also be categorical values.\n\n% A feature is a numeric attribute that characterizes one or more samples in a image, range scan or more generally a dataset. Features are used many domains including statistics, machine learning and computer vision. Features can be designed to only describe characteristics pertinent to the task at hand and be invariant to extraneous variables \\cite{Flach2012}. When analyzing images in computer vision it's desirable to have features that are invariant to lighting conditions and the orientation of the camera. Features without these properties are not very robust. \n\n% Unprocessed range scans describe positions of points in space, their relation to each other from the scanner perspective and the intensity of reflected light. The process of creating new descriptors from this data is called feature extraction \\cite{Jain1999}. Point normals are one type of descriptor that can be extracted from range scans \\cite{Klasing2009}. \n\n% Feature extraction is typically a preprocessing to a segmentation process. Because features are very useful in isolation, they are typically evaluated as part of a larger task \\cite{Jain1999}.\n\n\n% Data, information, knowledge\n\n\n% From an information theoretic point we cannot create information.\n\n% In computer vision features get rid of nuisances like lighting and transformations\n\n% How to pick or generate good features\n% 	Performance in an end to end task, together with a classifier and dataset\n\n% 		it tells us nothing on how a given feature can be improved, or how performance generalizes to different classifiers and different data sets.\n\n% Depends on the purpose\n\n% Correspondences, scale invariant, orientation invariant\n\n\n% \\todo{Feature extraction, feature selection}\n\n% Classification and grouping\n\n% \\subsubsection{Clustering}\n\n% Cluster analysis is the organization data points into groups (clusters) based on similarity \\cite{Jain1999}. Similarity is usually determined by grouping features into vector and comparing points via a distance metric in this multi dimensional space. Clustering is an instance of unsupervised learning in that the goal is to group unlabeled data into meaningful groups. The first step in a clustering algorithm is generally to select the most suitable features. Secondly a distance metric needs to be chosen in order to compare data points. Finally a grouping algorithm needs to be decided on. In order to efficiently represent clusters for further computation one may wish to extract a compact representation of the data. One could use the cluster centroid or compute a feature descriptor of the region. \n\n\n% Clustering divides things that are close into groups\n\n\n% membership criteria and\n\n% Euclidean distance is a common choice.\n\n% and normalize them if necessary\n\n\n\n% Clustering algorithms are hard to evaluate as \n\n% Validation\n\n\n% Issues\n% 	efficiency\n% 	normalization\n\n\n% Hard vs fuzzy partition\n\n% Hierarchical clustering\n\n% Graph theoretic clustering \n% 	Graph cuts\n% 	Markov random fields\n\n% Clustering is a useful ab\n\n\n% \\subsubsection{Classification}\n\n\n% % Unsupervised\n% % simulated annealing\n% % feature extraction\n% % HAVE NOT LOOKED AT SPLIT MERGE ALGORITHM\n\n\n\n% Classification is the problem of determining what class ($Y$) an observation belongs to given a set of characteristics ($X$) \\cite{Flach2012}. In our case this is determining whether an observed point in a range scan belongs to the wanted ($Y=wanted$) or unwanted ($Y=unwanted$) class given it's X Y Z and intensity values and relation to it's neighbors ($X$).\n\n% While our ultimate goal is to classify a point as wanted or unwanted, it is easier to classify a point as belonging to one of a larger number of subclasses \\cite{Malik2000}. For instance: trees, walls, person. The wanted/unwanted binary classification can then be expressed in terms of a point's membership to one of the intermediate classes. E.g. if a point belongs the tree class or person class, classify the point as unwanted, otherwise classify the point as wanted.\n\n% %  There are many possible partitions of an image into subsets. There may not be a correct one. Bayesian view - there are many possible interpretations in the context of prior world knowledge. Some of this knowledge is low level such as coherence of brightness or color, equally important is mid to high level knowledge about symmetries of objects or object models. Partitioning is hierarchical image segmentation based on low level queues cannot and should not aim to produce a complete final correct segmentation \\cite{Malik2000}\n\n% % Segmentaion is the task of decomposing the dataset into constituent pastrts that have some higher level meaning\n\n% % Unsupervised leaning is a good tool for silbing segmentation problem\n\n\n\n% The characteristics used to determine the class of a point is also commonly referred as a feature \\cite{Flach2012}. We will define a feature to be a piece of local information used in the classification task. The X Y Z \\& intensity values of each point are the most elementary features in datasets we focus on. These features can be combined or processed to create derivative features. Point normals are examples of derivative features. Point normals are not present in our data set but can estimated by considering the positions of points in a local neighborhood \\cite{Klasing2009}. Derivative features can simplify the classification problem by representing the data in a way that make it more meaningful and easier to relate a point to a class.\n\n% A classifier is a model used to solve a classification problem. Classifiers can be divided onto two groups namely heuristic classifiers, and machine learning classifiers. Heuristic classifiers are constructed by encoding expert knowledge needed to solve a classification problem\\cite{Clancey1985}. Machine learning algorithms construct a model from empirical data \\cite{Flach2012}.\n\n% Meshlab's denoising filter (see section \\ref{section:filters}) is an example of a simple heuristic classifier. The classifier encodes the knowledge that a point is likely to be noise if it has less than $n$ neighbors within radius $r$. Heuristic classifiers can be very effective but are often hard to maintain and are not very flexible \\cite{Flach2012}.\n\n% Machine learning methods classify points by using patterns in the data that are often difficult for humans detect or encode. There are two main types of machine learning algorithms. Supervised learning algorithms infers a classification function from a labeled training dataset. Unsupervised learning algorithms attempt to uncover the hidden structure in a dataset without a training set \\cite{Flach2012}.\n\n% % Proprietary systems likely use some machine learning for more advanced classification filters. The vegetation filter in VR Mesh Studio \\cite{VirtualGrid2012} could be achieved by training a supervised learning algorithm. The clustering filter in 3D Reshaper \\cite{Technodigit2012} appears to an unsupervised learning algorithm such as k-nearest neighbors.\n\n% % heuristic methods are not mutually exclusive with unsupervised leaning. Also called data mining.\n\n% Then main disadvantage of supervised learning is that skilled workers are required to create labeled training sets. Training data needs to be representative of the types of data one wishes to classify. There is a large variety of unwanted objects and terrains one could encounter in a scanning expedition. Applying a supervised learning approach to point cloud classification could thus require a large upfront investment depending on types of objects one wishes classify and the types of environments one expects to encounter. The effectiveness of the resulting classifier would be largely dependent on how similar the training sets are to new data.\n\n% Unsupervised learning algorithms differs from supervised learning in that it require no training data is used to construct a model. The model is created during classification task. The reduced human effort required aligns well with our goal of speeding up point cloud cleaning. Unsupervised learning however, cannot help us with applying a predefined set labeled to points. \\todo{Clustering is related to image segmentation} Clustering can however help us too divide a dataset into groups of related points called clusters \\cite{Flach2012}. How clusters are determined largely depends on what measure of relatedness is chosen. Unsupervised approaches are a good candidates for separating vegetation from structures.\n\n% Semi supervised learning is another class of learning algorithms that falls between supervised and unsupervised. Semi supervised learning leverages small amounts of labeled data to construct an initial model. This model can then be refined using unlabeled data \\cite{Flach2012}.\n\n% Our goal to find the features that best correlate with the types of points that needs to be preserved or removed. While doing so we also aim to keep the computational cost low as to maintain an interactive cleaning work flow. Our goals are similar for creating classifiers from these features.\n\n% \\section{Summary}\n\n% Terrestrial laser scanning is the primary tool used in digital heritage preservation for the documentation and 3d reconstruction. Triangulation scanners are more suited for scanning over short distances while TOF scanners work best for longer ranges. Traditional TOF scanners are slow but the phase based variation improves in speed while sacrificing range and potentially introducing noise.\n\n% Only TOF scanners are considered in this research. The data set returned from a TOF scanner varies between manufacturers. Datasets used here are in a grid structure and contain X, Y, Z coordinates as well the intensity of the reflected laser pulse. Manufacturers may also associate color with each point as well as raw scan data.\n\n% Laser scanner data sets need to be processed to create 3D models. This process includes cleaning, registration, meshing, hole filling and texturing.\n\n% During cleaning there are two types of noise that needs to be removed: instrumentation noise created by the scanner and environmental noise that is created by physical objects in the scene. Scanner manufacturers typically provide software that semi automatically remove instrumentation noise with varying levels of success. Removing environmental noise however is typically a more manual endeavor. The cleaning task is a very time consuming part of the reconstruction pipeline.\n\n% In this work we attempt to find ways to speed up the cleaning process through tooling and semi automated methods. We focus this on methods applicable to TOF scans in a grid form with X, Y, Z and intensity values.\n\n\n\n\n% Machine learning\n% Bayesian networks\n% Statistical approaches\n% Evolutionary approaches\n% Neural nets\n\n% Machine learning is a way of solving classification\n\n\n% \\begin{itemize}\n% \\item Point size\n% \\item Rendering depth vs intensity\n% \\item Navigation, fly through vs Track ball\n% \\item Exponential speed up in movement\n% \\end{itemize}\n\n\n% \\subsubsection*{CloudCompare}\n% Supports colour clouds\n% Rectangulkar, polygon select select tool\n% Hide and show layers\n\n\n% \\subsubsection{Meshlab}\n% Meshlab \\cite{Cignoni2008} is the only viable open source tool we know of. It's primary purpose is mesh processing but it does allow one to work with point clouds through various plug-ins. Some plug-ins lets one preview an action but once it is applied it cannot be undone as Meshlab does not support undo. Meshlab supports opening multiple point clouds in the same view port but tools can generally only operate on one at a time.\n\n% The viewport renders a track ball around the point cloud centre at allows one to rotate it around its axis.\n\n\n\n% \\subsection{Set operations}\n% Meshlab lets one invert selections\n\n% \\subsubsection{Z+F }\n\n% \\subsubsection{Point picking}\n% Point picking is the most basic tool available to us\n\n% \\subsubsection{Lasso}\n% Meshlab only rectangular\n\n% \\subsubsection{Brush tool}\n\n% \\subsubsection{Flood fill}\n\n% \\subsubsection{Plane selection}\n\n% \\subsubsection{Clustering}\n\n% \\subsubsection{Isolated point removal}\n\n% \\subsubsection{Limit box}\n\n\n\n% \\cite{Neisser1967}\n\n% \\subsection*{Summary}\n% Semi automated tools\n% Manual tools to correct imperfect results\n\n\n\n\n\n% The user is the ultimate \n\n% For a user to classify a dataset. The software should let the user label wanted or unwanted points as fast and as accurately as possible.\n\n% For a human to classify a point set, the data should to be visualized appropriately so that the human visual system can differentiate between wanted and unwanted points in a scan. \n\n% There are many decisions to make when visualizing a dataset. The most obvious way to render a range scan is in 3D. One can however also render a panoramic grid in 2D. Software exists that lets one do both \\cite{Z+F2014,VisualComputingLaboratory2012}. Both renderings have advantages and disadvantages. A 3D point cloud rendering lets one view more than one range scan at a time. Registering a set of scans can provide a more complete view of a scene that can help in identifying object. A single range scan only shows an incomplete scene from one perspective. The resolution of a scene also diminishes away from the origin.\n\n% \\begin{itemize}\n% \\item 2d vs 3d\n% \\item Colours\n% \\item Point size\n% \\item How to best present higher level features ala normals\n% \\item How to best hint at how far something is\n% \\end{itemize}\n\n% \\todo{Find visualization literature}\n% Range scans can be presented as either in 2 or 3 dimensional renderings.\n\n\n\n% A 3d rendering lets one view points from many perspectives. However, given the non uniform density of range scan, moving away from the scan origin may make objects harder to identify as points become more sparse. Moving away from the origin also reveals missing data caused by occlusions. Moving around a 3d scene may thus help identify points. A different perspective can however help specially delineate points that are close together from a 2d perspective.\n\n% A 2d rendering intensity values associated with each point of the scan grid provides a monotone panoramic view of the data without occlusions or apparent density issues. It is also possible to render the 2d grid as a depth map. Other derivative features may also be used on 2d and 3d visualizations to help a user classify points.\n\n% Given a navigable view, a user needs a way to label points seen on screen. The speed at which a user can label scan is primarily determined by how easily points can be identified in the visualization and how quickly a user can label the identified points by using a set of tools.\n\n% The most precise but also most laborious labeling tool would let the user label one point per interaction. The most easy to use tool would classify all points with a single interaction. However as a system performs more work on a user's behalf, a user sacrifices control over the task. In sacrificing control, precision could be lost. It is important when automating a task that speed gains make up for any accuracy loss.\n\n% Because automation is likely reduce accuracy, fully automated batch processing is not an option. As the user will have to continuously review and classification results and modify them to be in line with a mental image. It is thus important that interactivity be maintained.\n\n% Given the above considerations in the context provided in the previous chapters, relevant literature will be reviewed below.\n\n% \\section{Features}\n\n% Range scans as presented in the previous chapter, when rendered on screen, is processed by the human visual system. Light hits cone and rod cells in the eyes, which is sent to the visual cortex via the optic nerve. There the human brain process low level signals from eyes into higher level abstractions that eventually becomes meaningful representation a users mind. To classify a range scan this representation needs to be captured by software. As we cannot connect our brains directly to the our system bus, we this representation has to be transfered via a low bandwidth mouse and keyboard.\n\n% Our understanding of the human visual system means tools can be designed to guess what a user intends to label on screen. A system has access to the same information that is shown on screen and additional information that is not directly accessible by the human visual system. Exact positions of points in 3d space can only be inferred from a 2d representation while a system have exact 3d coordinates. Surface normals is another example of a feature the human visual system does not have direct access to. Many more higher level features can be used to better guess or augment a user's intent.\n\n% Higher level features are generally computed by looking at a point' neighbors. Neighbors can be located in the grid or by doing a search using 3d coordinate values. Both 2d and 3d computer vision techniques can thus be applied to range scans. Range scans have non uniform density 3d features need to be tolerant of this. Features from 2d computer vision are less affected by this.\n\n\n\n\n% \\section{Summary}\n\n% Terrestrial laser scanning is the primary tool used in digital heritage preservation for the documentation and 3d reconstruction. Triangulation scanners are more suited for scanning over short distances while TOF scanners work best for longer ranges. Traditional TOF scanners are slow but the phase based variation improves in speed while sacrificing range and potentially introducing noise.\n\n% Only TOF scanners are considered in this research. The data set returned from a TOF scanner varies between manufacturers. Datasets used here are in a grid structure and contain X, Y, Z coordinates as well the intensity of the reflected laser pulse. Manufacturers may also associate color with each point as well as raw scan data.\n\n% Laser scanner data sets need to be processed to create 3D models. This process includes cleaning, registration, meshing, hole filling and texturing.\n\n% During cleaning there are two types of noise that needs to be removed: instrumentation noise created by the scanner and environmental noise that is created by physical objects in the scene. Scanner manufacturers typically provide software that semi automatically remove instrumentation noise with varying levels of success. Removing environmental noise however is typically a more manual endeavor. The cleaning task is a very time consuming part of the reconstruction pipeline.\n\n% In this work we attempt to find ways to speed up the cleaning process through tooling and semi automated methods. We focus this on methods applicable to TOF scans in a grid form with X, Y, Z and intensity values.\n\n\n%  with some of the quirks related to a specific model. Some instrumentation noise can\n\n\n\n\n% We are limited in the data we have to clean\n% There are various types of noise\n% Noise created by the instrument\n% Noise created by the environment\n% We only focus on noise created by laser range scanners\n% There are cleaning and registration do not have to happen in order\n\n% \\section{Point cloud cleaning} \\label{sec:cleaning}\n% 	Focused specifically on the task of point cloud cleaning in heritage scenes\n\n% 	\\subsection{Problem}\n% 	Characterize heritage scenes\n% 	\\begin{itemize}\n% 		\\item Large scans\n% 		\\item many scans\n% 		\\item Non uniform density\n% 		\\item Large point sets\n% 		\\item Hard to distinguish trees from walls\n% 	\\end{itemize}\n\n% 	\\subsection{Existing systems}\n% 		\\begin{itemize}\n% 			\\item Z\\&Y\n% 			\\item Cyclone\n% 			\\item Pointools\n% 			\\item Meshlab\n% 			\\item VR Mesh Studio\n% 			\\item Carlson Pointcloud\n% 			\\item 3D Reshaper\n% 			\\item Terrascan\n% 		\\end{itemize}\n\n% 	\\subsection{Evaluation of existing systems}\n% 	We should look at existing systems in terms of a testing framework\n% 	Evaluate their tools\n% 	Evaluate user interface\n% 	\\begin{itemize}\n% 		\\item navigation: camera vs object move\n% 		\\item tool set (what tools are available)\n% 		\\item license\n% 		\\item 2d/3d editing\n% 		\\item extensibility (why did I not use it)\n% 	\\end{itemize}\n\n		\n\n% \\section{Summary}\n% %What do I conclude from all this and lead into the next chapter.	\n\n\n\n% \\section{Review of literature}\n\n% \\cite{Spina2010} Cultural heritage segmentation\n\n",
			"file": "chapters/02-background.tex",
			"file_size": 64415,
			"file_write_time": 130740124691964416,
			"settings":
			{
				"buffer_size": 64416,
				"line_ending": "Unix"
			}
		},
		{
			"file": "chapters/01-introduction.tex",
			"settings":
			{
				"buffer_size": 1912,
				"line_ending": "Unix"
			}
		},
		{
			"file": "thesis.tex",
			"settings":
			{
				"buffer_size": 2933,
				"line_ending": "Unix"
			}
		},
		{
			"file": "notes/cvbook.txt",
			"settings":
			{
				"buffer_size": 15333,
				"line_ending": "Unix"
			}
		},
		{
			"file": "chapters/problem.txt",
			"settings":
			{
				"buffer_size": 1271,
				"line_ending": "Unix",
				"name": "problem:"
			}
		},
		{
			"contents": "Genaral background that people need to understand it\n    Genral computer science persons what they need to understand\n\n    More important\n    \n\n\nLit review key paper\n    MAstery of the subject\n\n    Still gaps\n\n\n\nFrame work section\n    Design & implementation\n\n    UI\n        lll\n\n\n    Design goals\n        State\n\n    What options\n        Useable\n\n\n    Choosing features\n\n",
			"settings":
			{
				"buffer_size": 371,
				"line_ending": "Unix",
				"name": "Genaral background that people need to understand"
			}
		}
	],
	"build_system": "",
	"build_system_choices":
	[
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 392.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"inst",
				"Package Control: Install Package"
			],
			[
				"insta",
				"Package Control: Install Package"
			],
			[
				"remo",
				"Package Control: Remove Package"
			],
			[
				"clean",
				"LaTeXing: Clean up (Remove) all Nonessential Files"
			],
			[
				"clea",
				"LaTeXing: Clean Temporary Output Dictionary"
			],
			[
				"cac",
				"LaTeXing: Rebuild Cache"
			],
			[
				"build",
				"Build: Cancel"
			],
			[
				"latexing ca",
				"LaTeXing: Clean up (Remove) all Nonessential Files"
			],
			[
				"buil",
				"Build: Cancel"
			],
			[
				"clear",
				"LaTeXing: Clean up (Remove) all Nonessential Files"
			],
			[
				"cache",
				"LaTeXing: Rebuild Cache (Hard)"
			],
			[
				"istall ",
				"Package Control: Install Package"
			],
			[
				"remove",
				"Package Control: Remove Package"
			],
			[
				"instal",
				"Package Control: Install Package"
			]
		],
		"width": 530.0
	},
	"console":
	{
		"height": 139.0,
		"history":
		[
			"print(\"hello \" * 8)",
			"import hashlib",
			"import haslib",
			"import urllib.request,os,hashlib; h = '7183a2d3e96f11eeadd761d777e62404e330c659d4bb41d3bdf022e94cab3cd0'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://sublime.wbond.net/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by) ",
			"import urllib.request,os; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); open(os.path.join(ipp, pf), 'wb').write(urllib.request.urlopen( 'http://sublime.wbond.net/' + pf.replace(' ','%20')).read()) "
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"expanded_folders":
	[
		"/home/rickert/Masters/docs/Thesis",
		"/home/rickert/Masters/docs/Thesis/chapters",
		"/home/rickert/Masters/docs/Thesis/notes"
	],
	"file_history":
	[
		"/home/rickert/Masters/docs/misc/slides.txt",
		"/home/rickert/Masters/docs/Thesis/slides.txt",
		"/home/rickert/Masters/docs/Thesis/thesis.sublime-project",
		"/home/rickert/Masters/docs/Thesis/chapters/02-background.aux",
		"/home/rickert/Masters/docs/Thesis/thesis.tex",
		"/home/rickert/Masters/docs/Thesis/images/old/triangulate-laser-scan.ppm",
		"/home/rickert/Masters/docs/Thesis/images/structured-light-scan.xcf",
		"/home/rickert/Masters/docs/Thesis/chapters/literature.notes",
		"/home/rickert/Masters/docs/Thesis/chapters/06-conclusion.tex",
		"/home/rickert/Masters/docs/Thesis/02-background.aux",
		"/home/rickert/Masters/docs/Thesis/02-background.tex",
		"/home/rickert/Masters/docs/Thesis/01-introduction.tex",
		"/home/rickert/.config/sublime-text-3/Packages/LaTeXing/latexing/api/mendeley.map",
		"/home/rickert/Masters/cloudclean/CMakeLists.txt",
		"/home/rickert/Desktop/software.sh",
		"/home/rickert/Desktop/poster.txt",
		"/home/rickert/Workspace/OnlineForrest/data/dna-train.libsvm",
		"/home/rickert/Masters/cloudclean/plugins/edit_flood_pca/ui_settings.h"
	],
	"find":
	{
		"height": 38.0
	},
	"find_in_files":
	{
		"height": 102.0,
		"where_history":
		[
		]
	},
	"find_state":
	{
		"case_sensitive": false,
		"find_history":
		[
			"color",
			"center",
			"methods",
			"is",
			"neighborin",
			"Point",
			"point",
			"Point",
			"point",
			"Point",
			"point",
			"Point",
			"point",
			"Point",
			"point",
			"Point",
			"point",
			"Point",
			"point",
			"Point",
			"point",
			"Point",
			"point",
			"Point",
			"point",
			"Point",
			"point",
			"Point",
			"point",
			"Point",
			"point",
			"discrete",
			"neighboring",
			"laser",
			"IO plug",
			"as a",
			"FIG:S",
			"System",
			"system",
			"igure",
			"roll correc",
			"panoram",
			"LEAp",
			"present th",
			"tooling",
			"Hole f",
			"Heritage",
			"inter",
			"called",
			"range",
			"photo",
			"photogam",
			"init",
			"clean",
			"triangulation-scanners",
			"allow",
			"++(",
			"docum",
			"can",
			"[h]",
			"cite",
			"id",
			"op_smt_formtrain_req_reason",
			"93",
			"depart",
			"98",
			"93",
			"recv_training_man",
			"39",
			"log",
			"\n-",
			"=On",
			"-D",
			" ",
			"-D"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
			"acquire ",
			"aquire",
			"[ht]",
			"",
			":On",
			" On",
			"  ",
			"-D "
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 5,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "chapters/02-literature-review.tex",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 14493,
						"regions":
						{
						},
						"selection":
						[
							[
								7866,
								7866
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeXing/support/LaTeX.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1375.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "chapters/05-evaluation.tex",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 256,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeXing/support/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 8,
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "chapters/04-implementation.tex",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 8588,
						"regions":
						{
						},
						"selection":
						[
							[
								3605,
								3605
							]
						],
						"settings":
						{
							"spell_check": true,
							"syntax": "Packages/LaTeXing/support/LaTeX.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 143.0,
						"zoom_level": 1.0
					},
					"stack_index": 10,
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "chapters/06-conclusion.tex",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 91,
						"regions":
						{
						},
						"selection":
						[
							[
								84,
								84
							]
						],
						"settings":
						{
							"history_list_is_closing": true,
							"syntax": "Packages/LaTeXing/support/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 11,
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "chapters/03-design.tex",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 418,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeXing/support/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 4,
					"type": "text"
				},
				{
					"buffer": 5,
					"file": "chapters/03-software-framework.tex",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 38209,
						"regions":
						{
						},
						"selection":
						[
							[
								26824,
								26824
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeXing/support/LaTeX.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 1693.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"type": "text"
				}
			]
		},
		{
			"selected": 0,
			"sheets":
			[
				{
					"buffer": 6,
					"file": "chapters/02-background.tex",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 64416,
						"regions":
						{
						},
						"selection":
						[
							[
								9472,
								9472
							]
						],
						"settings":
						{
							"spell_check": true,
							"syntax": "Packages/LaTeXing/support/LaTeX.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 5239.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"type": "text"
				},
				{
					"buffer": 7,
					"file": "chapters/01-introduction.tex",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1912,
						"regions":
						{
						},
						"selection":
						[
							[
								536,
								536
							]
						],
						"settings":
						{
							"spell_check": true,
							"syntax": "Packages/LaTeXing/support/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"type": "text"
				},
				{
					"buffer": 8,
					"file": "thesis.tex",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2933,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeXing/support/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 510.0,
						"zoom_level": 1.0
					},
					"stack_index": 6,
					"type": "text"
				},
				{
					"buffer": 9,
					"file": "notes/cvbook.txt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 15333,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 891.0,
						"zoom_level": 1.0
					},
					"stack_index": 5,
					"type": "text"
				},
				{
					"buffer": 10,
					"file": "chapters/problem.txt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1271,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"auto_name": "problem:",
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 9,
					"type": "text"
				},
				{
					"buffer": 11,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 371,
						"regions":
						{
						},
						"selection":
						[
							[
								259,
								259
							]
						],
						"settings":
						{
							"auto_name": "Genaral background that people need to understand",
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 7,
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 31.0
	},
	"input":
	{
		"height": 33.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			],
			[
				1,
				0,
				2,
				1
			]
		],
		"cols":
		[
			0.0,
			0.5,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.exec":
	{
		"height": 112.0
	},
	"output.find_results":
	{
		"height": 0.0
	},
	"pinned_build_system": "",
	"project": "thesis.sublime-project",
	"replace":
	{
		"height": 58.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_symbol":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"selected_group": 1,
	"settings":
	{
	},
	"show_minimap": true,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 246.0,
	"status_bar_visible": true,
	"template_settings":
	{
	}
}
